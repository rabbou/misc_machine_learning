{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2053072e-12d7-48e4-8756-9d14fd9af4a7"
    }
   },
   "source": [
    "### Ruben Abbou\n",
    "\n",
    "## Word Embeddings using PMI\n",
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "4ec8122c-d90e-48a9-81d4-fa56d363c00c"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np, pickle\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "bec30b8b-bae8-47fc-ae95-cb9afed4d759"
    }
   },
   "outputs": [],
   "source": [
    "with open('/project2/cmsc25025/wikipedia/wiki-text.txt') as f:\n",
    "    for ws in f:\n",
    "        words = ws.split()\n",
    "stopwords = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "92c85ffc-a3a8-495c-b5ae-64082148cac1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab: 14989 words\n"
     ]
    }
   ],
   "source": [
    "words = [word for word in words if word not in stopwords]\n",
    "v = Counter(words)\n",
    "n = 414\n",
    "v = {key: val for key, val in v.items() if val > n}\n",
    "words = [word for word in words if word in v.keys()]\n",
    "N = len(v)\n",
    "print(\"Length of vocab: {0} words\".format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "7d753af5-7cbd-4f7a-befc-f5d2f07f1ebd"
    }
   },
   "outputs": [],
   "source": [
    "embedding_pairs = Counter()\n",
    "embedding_centers = Counter()\n",
    "Ns = 0\n",
    "for i in range(len(words)):\n",
    "    word = words[i]\n",
    "    lb = max(0, i-5)\n",
    "    ub = min(i+5, len(words)-1)\n",
    "    for k in range(lb, ub+1):\n",
    "        if k != i:\n",
    "            Ns+=1\n",
    "            embedding_centers[word] += 1\n",
    "            embedding_pairs[(word, words[k])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "407f8f69-ceb2-49f3-a70a-7a274952190e"
    }
   },
   "outputs": [],
   "source": [
    "M = csr_matrix([[log((embedding_pairs[(wi, wj)] + 1)*Ns / \\\n",
    "                   (embedding_centers[wi]*embedding_centers[wj])) for wi in v] for wj in v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = svds(M, k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.dot(U, np.sqrt(np.diag(s)))\n",
    "pickle.dump(W, open(\"matrix_W.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = pickle.load(open(\"matrix_W.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14989, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the closest vectors to a certain word, I took the difference between $W$ and the word's vector $v_w$, and then the norm of this difference across each vector, which results in a vector of $14,989$ norms for each vector of the matrix. Then I took the smallest 5 entries' indexes and obtained the resulting words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_5(word):\n",
    "    word_index = list(v).index(word)\n",
    "    diff_norms = np.linalg.norm(W-W[word_index], axis = 1)\n",
    "    indexes = list(diff_norms.argsort()[1:6])\n",
    "    print(\"The 5 closest words in the embedding space of %s are:\" % word)\n",
    "    for i in range(len(indexes)):\n",
    "        print(\"%i.\" %(i+1), \"%s\" % list(v)[indexes[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 closest words in the embedding space of physics are:\n",
      "1. mechanics\n",
      "2. quantum\n",
      "3. chemistry\n",
      "4. theoretical\n",
      "5. mathematics\n",
      "The 5 closest words in the embedding space of republican are:\n",
      "1. senator\n",
      "2. democrat\n",
      "3. democrats\n",
      "4. presidential\n",
      "5. candidate\n",
      "The 5 closest words in the embedding space of einstein are:\n",
      "1. relativity\n",
      "2. physicists\n",
      "3. paradox\n",
      "4. maxwell\n",
      "5. mechanics\n",
      "The 5 closest words in the embedding space of algebra are:\n",
      "1. algebraic\n",
      "2. finite\n",
      "3. theorem\n",
      "4. topology\n",
      "5. calculus\n",
      "The 5 closest words in the embedding space of fish are:\n",
      "1. fruit\n",
      "2. eggs\n",
      "3. eat\n",
      "4. seeds\n",
      "5. feed\n"
     ]
    }
   ],
   "source": [
    "print_top_5(\"physics\")\n",
    "print_top_5(\"republican\")\n",
    "print_top_5(\"einstein\")\n",
    "print_top_5(\"algebra\")\n",
    "print_top_5(\"fish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_5_analogy(w1, w2, w3, w4):\n",
    "    v1 = W[list(v).index(w1)]\n",
    "    v2 = W[list(v).index(w2)]\n",
    "    v3 = W[list(v).index(w3)]\n",
    "    new = v2-v1+v3\n",
    "    diff_norms = np.linalg.norm(W-new, axis = 1)\n",
    "    indexes = list(diff_norms.argsort()[:5])\n",
    "    print(\"Top 5 words that completes the analogy {0}:{1}::{2}:______\\nExpected: {3}\" \\\n",
    "          .format(w1, w2, w3, w4))\n",
    "    for i in range(len(indexes)):\n",
    "        print(\"%i.\" %(i+1), \"%s\" % list(v)[indexes[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words that completes the analogy france:paris::england:______\n",
      "Expected: london\n",
      "1. london\n",
      "2. oxford\n",
      "3. england\n",
      "4. cambridge\n",
      "5. edinburgh\n",
      "Top 5 words that completes the analogy hospital:doctor::college:______\n",
      "Expected: professor\n",
      "1. college\n",
      "2. school\n",
      "3. academy\n",
      "4. university\n",
      "5. professor\n",
      "Top 5 words that completes the analogy day:light::night:______\n",
      "Expected: dark\n",
      "1. light\n",
      "2. medium\n",
      "3. dark\n",
      "4. ground\n",
      "5. surface\n",
      "Top 5 words that completes the analogy winter:cold::summer:______\n",
      "Expected: hot/warm\n",
      "1. cold\n",
      "2. intense\n",
      "3. hot\n",
      "4. break\n",
      "5. cool\n",
      "Top 5 words that completes the analogy fall:leaves::spring:______\n",
      "Expected: flowers\n",
      "1. leaves\n",
      "2. fruit\n",
      "3. flowers\n",
      "4. leaf\n",
      "5. eggs\n",
      "Top 5 words that completes the analogy wine:alcohol::cocaine:______\n",
      "Expected: drug\n",
      "1. drugs\n",
      "2. cocaine\n",
      "3. drug\n",
      "4. chronic\n",
      "5. addiction\n"
     ]
    }
   ],
   "source": [
    "top_5_analogy(\"france\",\"paris\",\"england\", \"london\")\n",
    "top_5_analogy(\"hospital\",\"doctor\", \"college\", \"professor\")\n",
    "top_5_analogy(\"day\",\"light\",\"night\", \"dark\")\n",
    "top_5_analogy(\"winter\", \"cold\", \"summer\", \"hot/warm\")\n",
    "top_5_analogy(\"fall\", \"leaves\", \"spring\", \"flowers\")\n",
    "top_5_analogy(\"wine\", \"alcohol\", \"cocaine\", \"drug\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
